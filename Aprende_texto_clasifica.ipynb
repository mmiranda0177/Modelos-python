{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"C:\\Requerimientos.xlsx\", sheetname='Hoja1')\n",
    "\n",
    "\n",
    "print('Número de datos Cargados: {num}'.format(num=data.shape[0]))\n",
    "data = data.dropna()\n",
    "# Imprimimos el número de tweets a procesar\n",
    "print('Número de datos a procesar: {num}'.format(num=data.shape[0]))\n",
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby('REQU_TIRE_ID')['REQU_DETALLE'].count().reset_index().sort_values(by='REQU_DETALLE', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = data[data[\"REQU_TIRE_ID\"] != -1]\n",
    "data1.groupby('REQU_TIRE_ID')['REQU_DETALLE'].count().reset_index().sort_values(by='REQU_DETALLE', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data2 = data1.groupby('REQU_TIRE_ID')['REQU_DETALLE'].count().reset_index().sort_values(by='REQU_DETALLE', ascending=False)\n",
    "\n",
    "data2\n",
    "\n",
    "data3 = data2.loc[data2['REQU_DETALLE'] < 100].REQU_TIRE_ID.iloc[:].values\n",
    "data1 = data1[~data1.REQU_TIRE_ID.isin(data3)]\n",
    "\n",
    "data1.groupby('REQU_TIRE_ID')['REQU_DETALLE'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coment = [tuple(x) for x in data1[['REQU_TIRE_ID', 'REQU_DETALLE']].values]\n",
    "print('Número de comentarios Cargados: {num}'.format(num=len(coment)))\n",
    "# Carga de librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(coment[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from unicodedata import normalize\n",
    "texto = 'gasto medicaménto permanente'\n",
    "\n",
    "# -> NFD y eliminar diacríticos\n",
    "s1 = re.sub(\n",
    "        r\"([^n\\u0300-\\u036f]|n(?!\\u0303(?![\\u0300-\\u036f])))[\\u0300-\\u036f]+\", r\"\\1\", \n",
    "        normalize( 'NFD', X_norm[33]), 0, re.I\n",
    "    )\n",
    "\n",
    "# -> NFC\n",
    "s1 = normalize( 'NFC', s1)\n",
    "\n",
    "print( s1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt= 'Hola! Soy Ben Asistente Virtual de tu Seguro Integral tratare de ayudarte en lo que pueda.Que Te Importa \\xa0\\xa0Hola Por favor ingresa tu RUT sin puntos y con guion ejemplo 12345678-9Que Te Importa \\xa0\\xa018155047-3 Ingresa tu emailQue Te Importa \\xa0\\xa0bernarditaarias...'\n",
    "print(txt)\n",
    "#ignore = \n",
    "txt2 = txt.lower().replace('.','. ').replace('\\xa0','. ').replace('-','. ').strip()\n",
    "t1 = str.maketrans(dict.fromkeys('0123456789',\"\"))\n",
    "txt2 = txt2.translate(t1)\n",
    "\n",
    "sentencia = sent_tokenize(txt2)\n",
    "ignore_sen =  ['hola!','soy ben asistente virtual de tu seguro integral tratare de ayudarte en lo que pueda.', 'hola por favor ingresa tu rut sin puntos y con guion ejemplo .']\n",
    "\n",
    "txtfin = ''.join([grama for grama in sentencia if grama not in ignore_sen])# 'Soy Ben Asistente Virtual de tu Seguro Integral tratare de ayudarte en lo que pueda.'])\n",
    "print(txtfin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "import string\n",
    "import re\n",
    "from unicodedata import normalize\n",
    "import spacy_spanish_lemmatizer\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "nlp.replace_pipe(\"lemmatizer\", \"spanish_lemmatizer\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "en_stop = set(stopwords.words('spanish'))\n",
    "ignore_frase =  ['hola!', 'soy ben asistente virtual de tu seguro integral tratare de ayudarte en lo que pueda.', 'hola por favor ingresa tu rut sin puntos y con guion ejemplo .']\n",
    "ignore_words = ['estimados','estimadas','estimado','estimada','hola','favor','asegurado','asistente','tratare','juntos','siempre','ayudarte','virtual', 'indicar', 'procedimiento', 'digital', 'buenos','dias','rut', 'estimados','ayuda','bancochile','selecciona','ben', 'maritza','cl','rut','puntos','guion','email','ejemplo','sistema', 'nombre']\n",
    "e_palabra_caracter_intermedio = [\"@\",\"°\", \"º\"]\n",
    "e_palabra_caracter_inicial = [\"http\", \"https\", \"º\", \"°\"]\n",
    "e_palabra_caracter_final = [\".com\",\".cl\",\".org\",\"º\", \"°\"]\n",
    "\n",
    "\n",
    "#nlp = spacy.load('es_core_news_sm')\n",
    "\n",
    "# Divido los datos en dos listas \n",
    "#     X: los tweets\n",
    "#     y: target (polaridad)\n",
    "\n",
    "X = [doc[1] for doc in coment]\n",
    "y = np.array([doc[0] for doc in coment])\n",
    "\n",
    "def normalizar(sentenses):\n",
    "    #print(sentenses)\n",
    "    \"\"\"normalizamos la lista de frases y devolvemos la misma lista de frases normalizada\"\"\"\n",
    "    for index, sentense in enumerate(tqdm(sentenses)):\n",
    "        lema = ''\n",
    "        #print(sentense)\n",
    "        # Tokenizamos el tweets realizando los puntos 1,2 y 3.\n",
    "        if type(sentense) != int:\n",
    "            #print(sentense)\n",
    "            s = re.sub(\n",
    "                    r\"([^n\\u0300-\\u036f]|n(?!\\u0303(?![\\u0300-\\u036f])))[\\u0300-\\u036f]+\", r\"\\1\", \n",
    "                    normalize( 'NFD', sentense), 0, re.I\n",
    "                )\n",
    "            #print(sentense)\n",
    "            sentense = normalize( 'NFC', s)\n",
    "            regex = re.compile(\"|\".join(f\"\\S+{cad}\\S+\" for cad in e_palabra_caracter_intermedio))\n",
    "            sentense = regex.sub(\"\", sentense)\n",
    "            regex = re.compile(\"|\".join(f\"{cad}\\S+\" for cad in e_palabra_caracter_inicial))\n",
    "            sentense = regex.sub(\"\", sentense)\n",
    "            regex = re.compile('|'.join(f\"\\S+{cad}\" for cad in e_palabra_caracter_final))\n",
    "            sentense = regex.sub(\"\", sentense).replace(\"  \", \" \")\n",
    "            t = str.maketrans(dict.fromkeys('0123456789',\"\"))\n",
    "            sentense = sentense.lower().replace('.','. ').replace('\\xa0','. ').replace('-','. ').strip()\n",
    "            #print(sentense)\n",
    "            sentense = sentense.translate(t)\n",
    "            #print(sentense)\n",
    "            sentense = sent_tokenize(sentense)\n",
    "            sentense = ''.join([fra for fra in sentense if fra not in ignore_frase])\n",
    "            sentense = sentense.lower().replace('.',' ').replace('-','').replace('\"','').replace('º','').replace('°','').strip()\n",
    "            sentense = sentense.translate(t)\n",
    "            word_tokens = word_tokenize(sentense)\n",
    "            #print(tokenizer.tokenize(sentense[0]))\n",
    "            #print(sentense)\n",
    "            # Puntos 4,5 y 6\n",
    "            sentense = ' '.join([word for word in word_tokens if word not in ignore_words \n",
    "                                        and word not in en_stop\n",
    "                                        and word not in string.punctuation])\n",
    "            #word_grama = word_tokenize(sentense)\n",
    "            #sentenses[index] = list(nltk.ngrams(word_grama,2))\n",
    "            #print(sentense)\n",
    "            for token in nlp(sentense):\n",
    "                lema = lema + ' ' + token.lemma_\n",
    "            #print(lema)\n",
    "            sentenses[index] = lema\n",
    "            #print(sentenses[index])\n",
    "        #(not word.is_punct)\n",
    "         #                    and (not word.text.startswith('@'))\n",
    "          #                   and (not word.text.startswith('http'))\n",
    "           #                  and (not ':' in word.text)])\n",
    "    return sentenses\n",
    "\n",
    "# Normalizamos las frases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "seed = 7\n",
    "validation_size = 0.2\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_norm,y,test_size=validation_size, shuffle=True, random_state=seed)\n",
    "print('comentarios de entrenamiento: {}'.format(len(X_train)))\n",
    "print('comentarios de Test: {}'.format(len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train.astype(int)\n",
    "np.array(X_train, dtype=np.str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCT_TEST = 0.25\n",
    "n_tail = len(X_norm) - int(len(X_norm) * PCT_TEST)\n",
    "print('Corte en el comentario número {} de los {} tweets del Dataset.'.format(n_tail, len(X_norm)))\n",
    "\n",
    "X_train = X_norm[:n_tail]\n",
    "y_train = y[:n_tail]\n",
    "X_test = X_norm[n_tail:]\n",
    "y_test = y[n_tail:]\n",
    "\n",
    "print('comentarios de entrenamiento: {}'.format(len(X_train)))\n",
    "print('comentarios de Test: {}'.format(len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "\n",
    "# Hacemos un one-hot encoder del texto\n",
    "VOCAB_SIZE = 100000 # Poner un valor muy alto\n",
    "X_train = [one_hot(doc, VOCAB_SIZE) for doc in np.array(X_train, dtype=np.str)]\n",
    "X_test = [one_hot(doc, VOCAB_SIZE) for doc in np.array(X_test, dtype=np.str)]\n",
    "\n",
    "# Codificación del Target\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_train)\n",
    "y_train = encoder.transform(y_train)\n",
    "#encoder.fit(y_test)\n",
    "y_test = encoder.transform(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creación de secuencia de palabras\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MAX_WORDS_COMENT = 32\n",
    "pad_corpus_train = pad_sequences(X_train, maxlen=MAX_WORDS_COMENT, padding='post')\n",
    "pad_corpus_test = pad_sequences(X_test, maxlen=MAX_WORDS_COMENT, padding='post')\n",
    "print(pad_corpus_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_test)\n",
    "import keras.backend as K\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM, BatchNormalization, GRU, Bidirectional\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import adam\n",
    "from keras.layers import Embedding\n",
    "\n",
    "EMBEDDING_SIZE = \n",
    "# Set Model\n",
    "model1 = Sequential()\n",
    "model1.add(Embedding(VOCAB_SIZE, EMBEDDING_SIZE))\n",
    "model1.add(Bidirectional(GRU(EMBEDDING_SIZE)))\n",
    "#model1.add(LSTM(128, return_sequences=True))\n",
    "model1.add(Dropout(0.2))\n",
    "model1.add(BatchNormalization())\n",
    "\n",
    "# Set Optimizer\n",
    "opt = adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=1e-6)\n",
    "\n",
    "# Compile model\n",
    "model1.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=opt,\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Input, LSTM, GRU, SimpleRNN, Bidirectional, BatchNormalization\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "EMBEDDING_SIZE = 32\n",
    "opt = Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=1e-6)\n",
    "model = Sequential()\n",
    "model.add(Embedding(VOCAB_SIZE, EMBEDDING_SIZE))\n",
    "#model.add(GRU(EMBEDDING_SIZE, return_sequences=True))\n",
    "model.add(Bidirectional(GRU(EMBEDDING_SIZE)))\n",
    "model.add(Dropout(0.2))\n",
    "#model.add(GRU(32))\n",
    "#model.add(Embedding(100000, EMBEDDING_SIZE))\n",
    "#model.add(GRU(EMBEDDING_SIZE))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(16, activation='softmax'))\n",
    "#model.compile(loss='categorical_crossentropy', optimizer=opt)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "# Imprimimos la arquitectura de la red\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "now = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "os.makedirs('tensorboard_logs/{}'.format(now))\n",
    "path = os.path.join('tensorboard_logs', now)\n",
    "\n",
    "history=model.fit(pad_corpus_train, \n",
    "                  y_train, \n",
    "                  batch_size=16, \n",
    "                  epochs=10, \n",
    "                  validation_data=(pad_corpus_test, y_test), \n",
    "                  verbose=2,\n",
    "                  callbacks=[TensorBoard(log_dir=path,\n",
    "                                         histogram_freq=2,\n",
    "                                         write_graph=True,\n",
    "                                         write_images=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Pintamos las métricas por epoch\n",
    "def plot_metric(history, name, remove_first=0):\n",
    "    metric_train = np.array(history.history[name])[remove_first:]\n",
    "    metric_test = np.array(history.history['val_{}'.format(name)])[remove_first:]\n",
    "    acum_avg_metric_train = (np.cumsum(metric_train) / (np.arange(metric_train.shape[-1]) + 1))[remove_first:]\n",
    "    acum_avg_metric_test = (np.cumsum(metric_test) / (np.arange(metric_test.shape[-1]) + 1))[remove_first:]\n",
    "    plt.figure(figsize=(20, 8))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title('{} - Epochs'.format(name))\n",
    "    plt.plot(metric_train, label='{} Train'.format(name))\n",
    "    plt.plot(metric_test, label='{} Test'.format(name))\n",
    "    plt.grid()\n",
    "    plt.legend(loc='upper center')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title('AVG ACCUMULATIVE {} - Epochs'.format(name))\n",
    "    plt.plot(acum_avg_metric_train, label='{} Train'.format(name))\n",
    "    plt.plot(acum_avg_metric_test, label='{} Test'.format(name))\n",
    "    plt.grid()\n",
    "    plt.legend(loc='upper center')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función de perdida\n",
    "plot_metric(history=history, name='loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "plot_metric(history=history, name='acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, classification_report\n",
    "\n",
    "y_true = encoder.inverse_transform(y_test.reshape(-1))\n",
    "y_pred = model.predict_classes(pad_corpus_test)\n",
    "y_pred = encoder.inverse_transform(y_pred.reshape(-1))\n",
    "\n",
    "print('Accuracy: {acc:0.4f}'.format(acc=accuracy_score(y_true=y_true, y_pred=y_pred)))\n",
    "print('F1: {f1:0.4f}'.format(f1=f1_score(y_true=y_true, y_pred=y_pred, average='weighted')))\n",
    "print('Precision: {pre:0.4f}'.format(pre=precision_score(y_true=y_true, y_pred=y_pred, average='weighted')))\n",
    "print('Recall: {rec:0.4f}'.format(rec=recall_score(y_true=y_true, y_pred=y_pred, average='weighted')))\n",
    "print(classification_report(y_true=y_true, y_pred=y_pred))\n",
    "labels = [1001,1005,2000,2036,2046,2051,2058,2072,2075,2124,2129,4029,4030,4035,4039]\n",
    "confusion_matrix = confusion_matrix(y_true=y_true, y_pred=y_pred, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, title, cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], 'd'), horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "# Pintamos la matriz de confusión\n",
    "plt.figure(figsize=(12, 6))\n",
    "plot_confusion_matrix(confusion_matrix, classes=labels, title='Matriz de Confusión')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model_lstm_32_16_lr_0001_32_16_lematiza')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
